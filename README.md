# ğŸ­ Multimodal Emotion Recognition

## ğŸ“Œ Overview

This project implements a **Multimodal Emotion Recognition System** using:

- ğŸ¤ Speech-only input  
- ğŸ“ Text-only input  
- ğŸ”€ Fusion of Speech + Text

The system is built using Deep Learning models to analyze emotional patterns from audio signals and textual content.

Dataset used: **Toronto Emotional Speech Set (**TESS**)** Available on Kaggle.

---

## ğŸ¯ Objective

To build and compare three models:

## Speech-only emotion recognition

## Text-only emotion recognition ## Multimodal (Speech + Text) fusion model

And analyze:
- Which emotions are easiest/hardest to classify
- When fusion improves performance
- Cluster separability using t-**SNE** visualizations

---

## ğŸ—‚ï¸ Project Structure

project/ â”‚ â”œâ”€â”€ models/ â”‚ â”œâ”€â”€ speech_pipeline/ â”‚ â”‚ â”œâ”€â”€ train.py â”‚ â”‚ â””â”€â”€ test.py â”‚ â”‚ â”‚ â”œâ”€â”€ text_pipeline/ â”‚ â”‚ â”œâ”€â”€ train.py â”‚ â”‚ â””â”€â”€ test.py â”‚ â”‚ â”‚ â””â”€â”€ fusion_pipeline/ â”‚ â”œâ”€â”€ train.py â”‚ â””â”€â”€ test.py â”‚ â”œâ”€â”€ Results/ â”‚ â”œâ”€â”€ accuracy_tables.csv â”‚ â””â”€â”€ plots/ â”‚ â”œâ”€â”€ accuracy_comparison.png â”‚ â”œâ”€â”€ tsne_speech.png â”‚ â”œâ”€â”€ tsne_text.png â”‚ â””â”€â”€ tsne_fusion.png â”‚ â”œâ”€â”€ Multimodal_Emotion_Recognition_Report.pdf â”œâ”€â”€ **README**.md â””â”€â”€ requirements.txt

---

## ğŸ§  Model Architectures

### ğŸ”¹ 1. Speech Pipeline

**Preprocessing**
- Resampled to 16kHz
- Fixed length padding (3 seconds)
- **MFCC** feature extraction (40 coefficients)

**Architecture**
- **CNN** (local acoustic features)
- BiLSTM (temporal emotional patterns)
- Fully Connected + Softmax classifier

---

### ğŸ”¹ 2. Text Pipeline

**Preprocessing**
- Extracted word from filename
- Tokenized using **BERT** tokenizer

**Architecture**
- **BERT** (bert-base-uncased)
- Dropout
- Fully Connected layer
- Softmax classifier

---

### ğŸ”¹ 3. Fusion Pipeline

- Speech embedding (**CNN** + BiLSTM)
- Text embedding (**BERT**)
- Concatenation
- Fully Connected layers
- Softmax classifier

---

## ğŸ“Š Experimental Results

| Model        | Test Accuracy |
|--------------|--------------|
| Speech-only  | 86.07% |
| Text-only    | 13.21% |
| Fusion       | 57.50% |

---

## ğŸ“ˆ Visualization

### ğŸ”¹ Model Comparison

`accuracy_comparison.png`

Speech > Fusion > Text

### ğŸ”¹ t-SNE Visualizations

- `tsne_speech.png` â†’ Clear emotional clustering  
- `tsne_text.png` â†’ Poor separation (text lacks emotion)  
- `tsne_fusion.png` â†’ Moderate clustering

---

## ğŸ” Analysis

### âœ… Easiest Emotions

- Happy
- Disgust
- Angry

Strong acoustic variations make them easier to classify.

### âŒ Hardest Emotions

- Neutral
- Fear

Subtle acoustic differences lead to confusion.

### ğŸ”€ When Does Fusion Help?

Fusion improves performance compared to text-only model. However, since **TESS** contains neutral spoken words, text adds limited emotional information, so fusion does not outperform speech-only model.

### âš ï¸ Error Analysis

## Fear misclassified as Angry due to similar high pitch.

## Neutral confused with Sad due to low energy overlap. ## Fusion model misclassified some classes because text introduced noise. ## Class imbalance influenced predictions in some cases.

---

## âš™ï¸ Installation

Clone repository:

git clone [https://github.com/kattabhavana9/Multimodal-Emotion-Recognition.git](https://github.com/kattabhavana9/Multimodal-Emotion-Recognition.git) cd Multimodal-Emotion-Recognition

Install dependencies:

pip install -r requirements.txt

---

## â–¶ï¸ How to Run

### ğŸ¤ Speech Model

cd models/speech_pipeline python train.py python test.py

### ğŸ“ Text Model

cd models/text_pipeline python train.py python test.py

### ğŸ”€ Fusion Model

cd models/fusion_pipeline python train.py python test.py

---

## ğŸ“¦ Dependencies

- torch
- torchaudio
- librosa
- transformers
- scikit-learn
- matplotlib
- numpy
- tqdm

---

## ğŸ“„ Report

The complete detailed report is available in:

Multimodal_Emotion_Recognition_Report.pdf

---

## ğŸš€ Author

**Bhavana Katta** Multimodal Emotion Recognition â€“ Assignment 2
